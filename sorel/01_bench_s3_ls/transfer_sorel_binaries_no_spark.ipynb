{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer SOREL Binaries without Spark\n",
    "\n",
    "Use ```pyarrow``` https://arrow.apache.org/docs/python/\n",
    "\n",
    "Other python libraries:  https://stackoverflow.com/questions/32940416/methods-for-writing-parquet-files-using-python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from typing import List\n",
    "import datetime\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source\n",
    "BUCKET = 'sorel-20m'\n",
    "PREFIX = '09-DEC-2020/binaries'\n",
    "BUCKET_PATH = f's3://{BUCKET}'\n",
    "SRC_PATH = f'{BUCKET_PATH}/{PREFIX}'\n",
    "\n",
    "MB = 1024 * 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AWS_PROFILE'] = 'personal'  # Needed by pyarrow\n",
    "session = boto3.Session(profile_name='personal')\n",
    "s3 = session.resource('s3')\n",
    "\n",
    "def get_objects(filter_chars: str, bucket: str, prefix: str) -> List[str]:\n",
    "    bucket = s3.Bucket(bucket)\n",
    "    prefix = f'{prefix}/{filter_chars}'\n",
    "    objects = bucket.objects.filter(Prefix=prefix)\n",
    "    return objects\n",
    "\n",
    "def get_num_objects(filter_chars):\n",
    "    objects = get_objects(filter_chars, BUCKET, PREFIX)\n",
    "    object_list = [o for o in objects]\n",
    "    return len(object_list)\n",
    "\n",
    "# TODO Use logger https://docs.python.org/3/howto/logging.html\n",
    "def logmsg(msg):\n",
    "    print(msg)\n",
    "\n",
    "def logmsg01(msg):\n",
    "    logmsg(f'{datetime.datetime.now():%Y-%m-%d %H:%M:%S}:  {msg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_chars = '0000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-21 23:02:17:  Begins\n",
      "2023-03-21 23:02:23:  i=10, key=09-DEC-2020/binaries/0000147e127de86fdec59ebafb0cca8d3002dc3513ab22f21c646fb7185a48f7, cumulative size=4456296\n",
      "2023-03-21 23:02:28:  Writing 18 keys with content.  Size=11411026.\n",
      "2023-03-21 23:02:43:  i=20, key=09-DEC-2020/binaries/00001c81777f5519a32efe91ac906f6b85adae2fc15b8884a35ef38f73dbb4d4, cumulative size=1917716\n",
      "2023-03-21 23:02:52:  Writing 6 keys with content.  Size=14218896.\n",
      "2023-03-21 23:03:21:  i=30, key=09-DEC-2020/binaries/000028e58d29d53833b17255293b73726c85f385e0052d4bddcb998311bdc5f7, cumulative size=5089559\n",
      "2023-03-21 23:03:32:  Writing 16 keys with content.  Size=10530652.\n",
      "2023-03-21 23:03:41:  i=40, key=09-DEC-2020/binaries/000031498f0ce7879b833b9fb180285a8649878cc556bb55afe1e18af52e4df5, cumulative size=0\n",
      "2023-03-21 23:03:52:  i=50, key=09-DEC-2020/binaries/00003cb2964261e0125562a5728c7e3e1035c287659840a60e0fc6d8ae63fb5b, cumulative size=6583043\n",
      "2023-03-21 23:03:57:  Writing 19 keys with content.  Size=10692410.\n",
      "2023-03-21 23:04:08:  i=60, key=09-DEC-2020/binaries/00004b7ecdd3c44a8b3aea00b7709e45a3bc0b258a7b63bbc8578b519e10fdcd, cumulative size=42950\n",
      "2023-03-21 23:04:25:  i=70, key=09-DEC-2020/binaries/00006dec7e51ad46b91d2ff2afac00852c8dbae850c808f8bcda120866950d86, cumulative size=9509860\n",
      "2023-03-21 23:04:27:  Writing 13 keys with content.  Size=11688192.\n",
      "2023-03-21 23:04:54:  i=80, key=09-DEC-2020/binaries/00007f573fd967dcb8795df2bbae6a8b562b1dbd2b228ed84934a57868b25358, cumulative size=4653604\n",
      "2023-03-21 23:05:03:  Writing 15 keys with content.  Size=10584926.\n",
      "2023-03-21 23:05:20:  i=90, key=09-DEC-2020/binaries/000089f368618e90d69100c57440963f25f7b7f53ba5bc0d8c024f2a35eb3963, cumulative size=5143840\n",
      "2023-03-21 23:05:25:  Writing 10 keys with content.  Size=12352305.\n",
      "2023-03-21 23:05:39:  i=100, key=09-DEC-2020/binaries/00009a831614a0246a95007122b93901997dd85e330816e059a8001c96f16c69, cumulative size=1181692\n",
      "2023-03-21 23:05:48:  Writing 10 keys with content.  Size=11116180.\n",
      "2023-03-21 23:06:13:  i=110, key=09-DEC-2020/binaries/00009f80c52ebfb35bb8f6a3cab7c0e87168fac0b58ce4c9d452c5dfe3ebabf5, cumulative size=4510247\n",
      "2023-03-21 23:06:13:  Writing 4 keys with content.  Size=11956622.\n",
      "2023-03-21 23:06:43:  i=120, key=09-DEC-2020/binaries/0000ae2c77d1a34dca2a0739a908d2be1d32ff9f6da70c8b4980fcf320bdfdad, cumulative size=6158046\n",
      "2023-03-21 23:06:46:  Writing 11 keys with content.  Size=11809414.\n",
      "2023-03-21 23:07:14:  Writing 3 keys with content.  Size=22756999.\n",
      "2023-03-21 23:07:43:  i=130, key=09-DEC-2020/binaries/0000bf191f1d873d5538805642557a7c2b0d8a2adf854e759761c9f3c0d2ebc3, cumulative size=2816558\n",
      "2023-03-21 23:07:51:  Writing 9 keys with content.  Size=19058893.\n",
      "2023-03-21 23:08:11:  i=140, key=09-DEC-2020/binaries/0000c7b763c8f2d7588ad9dd1c912dca9e1a28621f0d4dd1ea1447665a96820f, cumulative size=3734689\n",
      "2023-03-21 23:08:18:  i=150, key=09-DEC-2020/binaries/0000dfb6256443a5a21ec3d9f0fd34552a0b115220890fbc96027a2ee6b4c616, cumulative size=10165597\n",
      "2023-03-21 23:08:18:  Writing 17 keys with content.  Size=10573565.\n",
      "2023-03-21 23:08:32:  i=160, key=09-DEC-2020/binaries/0000f08a3eba53b852a736d06c735f07b588c1c75ac78428b6054c1a1c84bee3, cumulative size=1309900\n",
      "2023-03-21 23:08:43:  i=170, key=09-DEC-2020/binaries/0000f983330947f8d56fa2596889095467c2bf31624133370c35796f0752fd1d, cumulative size=9388859\n",
      "2023-03-21 23:08:43:  Writing 21 keys with content.  Size=10563833.\n",
      "2023-03-21 23:09:11:  Writing 3 keys with content.  Size=31842788.\n",
      "2023-03-21 23:09:36:  Complete.  Processed 175 files.  Total size=211156701.  Elapsed=0:07:19.758194\n"
     ]
    }
   ],
   "source": [
    "objects = get_objects(filter_chars, BUCKET, PREFIX)\n",
    "num_objects = len([o for o in objects])\n",
    "\n",
    "#max_size = 1024 * MB\n",
    "max_size = 10 * MB\n",
    "cur_size = 0\n",
    "cur_num_rec = 0\n",
    "tot_size = 0\n",
    "\n",
    "delete_target = True\n",
    "\n",
    "# TODO Add logmsg01\n",
    "logmsg01('Begins')\n",
    "start_time = datetime.datetime.now()\n",
    "d = {'key': [],\n",
    "        'last_modified': [],\n",
    "        'size': [],\n",
    "        'content': []}\n",
    "for i, o in enumerate(objects):\n",
    "    num_rec = i + 1\n",
    "    cur_num_rec += 1\n",
    "    # Read from S3\n",
    "    o.load()\n",
    "    response = o.get()\n",
    "    content = response['Body'].read()\n",
    "    #content = None\n",
    "    d['key'].append(o.key)\n",
    "    d['last_modified'].append(o.last_modified)\n",
    "    d['size'].append(o.size)\n",
    "    d['content'].append(content)\n",
    "    if i and i % 10 == 0:\n",
    "        logmsg01(f'i={i}, key={o.key}, cumulative size={cur_size}')\n",
    "    cur_size += o.size \n",
    "    tot_size += o.size\n",
    "    # if num_rec > 5:\n",
    "    #     break\n",
    "\n",
    "    # TODO Write file before size exceeds the limit.  However, if a single file is larger than the max, write it.\n",
    "    if cur_size > max_size or num_rec == num_objects:\n",
    "    # Write to parquet\n",
    "        table = pa.Table.from_pydict(d)\n",
    "\n",
    "        logmsg01(f'Writing {cur_num_rec} keys with content.  Size={cur_size}.')\n",
    "        # https://stackoverflow.com/questions/58818227/how-to-write-pyarrow-parquet-data-to-s3-bucket\n",
    "        pq.write_to_dataset(table, \n",
    "                            root_path=f's3://sorel-20m-demo/output_no_spark/', \n",
    "                            #filesystem=s3,\n",
    "                            existing_data_behavior=\"delete_matching\" if delete_target else \"overwrite_or_ignore\",\n",
    "                            compression='snappy')\n",
    "        delete_target = False\n",
    "        cur_size = 0\n",
    "        cur_num_rec = 0\n",
    "        d = {'key': [],\n",
    "                'last_modified': [],\n",
    "                'size': [],\n",
    "                'content': []}\n",
    "end_time = datetime.datetime.now()        \n",
    "logmsg01(f'Complete.  Processed {num_rec} files.  Total size={tot_size}.  Elapsed={end_time - start_time}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GUIAutomation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
