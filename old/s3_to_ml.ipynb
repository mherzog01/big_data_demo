{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d33410-af8a-44d0-a481-87133cad26f8",
   "metadata": {},
   "source": [
    "# Full Pipeline - Few Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a17bb5-2da7-4244-a3ba-aef0546c2b04",
   "metadata": {},
   "source": [
    "Assume that we are using the PySpark kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a22f229-e8f7-40a8-a392-9c3cf6b82b52",
   "metadata": {},
   "source": [
    "## Read files recursively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed1c0d1d-debc-47c5-bc87-2f457a7fca36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T19:06:47.039150Z",
     "iopub.status.busy": "2022-04-21T19:06:47.038906Z",
     "iopub.status.idle": "2022-04-21T19:06:47.107686Z",
     "shell.execute_reply": "2022-04-21T19:06:47.104663Z",
     "shell.execute_reply.started": "2022-04-21T19:06:47.039125Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b536b7f3a13a4115b62f57b5290c76e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.ml.linalg import DenseVector, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8fcfa2a-e559-4d3a-a055-1a89130b4a38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T19:06:48.748366Z",
     "iopub.status.busy": "2022-04-21T19:06:48.748136Z",
     "iopub.status.idle": "2022-04-21T19:06:49.018667Z",
     "shell.execute_reply": "2022-04-21T19:06:49.017800Z",
     "shell.execute_reply.started": "2022-04-21T19:06:48.748343Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39b55aaa9cf49358cdb38e3829a1982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "this version of pandas is incompatible with numpy < 1.17.3\n",
      "your numpy version is 1.16.5.\n",
      "Please upgrade numpy to >= 1.17.3 to use this pandas version\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/pandas/__init__.py\", line 22, in <module>\n",
      "    from pandas.compat import (\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/pandas/compat/__init__.py\", line 15, in <module>\n",
      "    from pandas.compat.numpy import (\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/pandas/compat/numpy/__init__.py\", line 27, in <module>\n",
      "    f\"this version of pandas is incompatible with numpy < {_min_numpy_ver}\\n\"\n",
      "ImportError: this version of pandas is incompatible with numpy < 1.17.3\n",
      "your numpy version is 1.16.5.\n",
      "Please upgrade numpy to >= 1.17.3 to use this pandas version\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37f7d030-fbe8-401f-9d79-bf580a5b3efd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T19:06:49.964125Z",
     "iopub.status.busy": "2022-04-21T19:06:49.963758Z",
     "iopub.status.idle": "2022-04-21T19:06:50.241581Z",
     "shell.execute_reply": "2022-04-21T19:06:50.240672Z",
     "shell.execute_reply.started": "2022-04-21T19:06:49.964086Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377dc70ae248470e917bb20407a6afda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Matplotlib requires numpy>=1.17; you have 1.16.5\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/matplotlib/__init__.py\", line 208, in <module>\n",
      "    _check_versions()\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/matplotlib/__init__.py\", line 204, in _check_versions\n",
      "    raise ImportError(f\"Matplotlib requires {modname}>={minver}; \"\n",
      "ImportError: Matplotlib requires numpy>=1.17; you have 1.16.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e93621ae-b94b-4f88-8e13-eb6016488463",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T19:06:51.198335Z",
     "iopub.status.busy": "2022-04-21T19:06:51.198088Z",
     "iopub.status.idle": "2022-04-21T19:06:58.510165Z",
     "shell.execute_reply": "2022-04-21T19:06:58.509452Z",
     "shell.execute_reply.started": "2022-04-21T19:06:51.198310Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3277c399a0472f9fc4664317b6d596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://sparkbyexamples.com/spark/spark-read-binary-file-into-dataframe/\n",
    "df = spark.read.format(\"image\").option(\"recursiveFileLookup\", True).load(\"s3://multimedia-commons/data/images/000/\")\n",
    "#df = spark.read.format(\"image\").option(\"recursiveFileLookup\", True).load(\"s3://multimedia-commons/data/images/00*\")\n",
    "#df = spark.read.format(\"image\").option(\"recursiveFileLookup\", True).load(\"s3://multimedia-commons/data/images/{00*,01*}\")\n",
    "#df = spark.read.format(\"image\").option(\"recursiveFileLookup\", True).load(\"s3://multimedia-commons/data/images/{00*,01*}\").persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80d0dc82-5fa8-4223-ae03-4540f866673c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T19:06:58.511680Z",
     "iopub.status.busy": "2022-04-21T19:06:58.511435Z",
     "iopub.status.idle": "2022-04-21T19:06:58.782442Z",
     "shell.execute_reply": "2022-04-21T19:06:58.781776Z",
     "shell.execute_reply.started": "2022-04-21T19:06:58.511646Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04c179d11494930bdd159c5055b50ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16dc4946-b251-49fe-a263-4c71b2404ad2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T19:06:58.783671Z",
     "iopub.status.busy": "2022-04-21T19:06:58.783502Z",
     "iopub.status.idle": "2022-04-21T19:07:36.402936Z",
     "shell.execute_reply": "2022-04-21T19:07:36.402260Z",
     "shell.execute_reply.started": "2022-04-21T19:06:58.783650Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0857b6edce46478f941258ba3fc95243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images found = 5"
     ]
    }
   ],
   "source": [
    "print(f'Number of images found = {df.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023d35f0-fc4c-4646-aebb-a25382ec7937",
   "metadata": {},
   "source": [
    "## Convert image to NDArray\n",
    "*Runs quickly for large amounts of data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3c4b829-8fd1-4ce0-83f7-7776896a0f29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T19:07:36.404477Z",
     "iopub.status.busy": "2022-04-21T19:07:36.404200Z",
     "iopub.status.idle": "2022-04-21T19:07:49.762790Z",
     "shell.execute_reply": "2022-04-21T19:07:49.762208Z",
     "shell.execute_reply.started": "2022-04-21T19:07:36.404442Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2cab3c3356f4fe0be53dd219581a870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image fields = ['origin', 'height', 'width', 'nChannels', 'mode', 'data']\n",
      "+--------------------+--------------------+\n",
      "|               image|                vecs|\n",
      "+--------------------+--------------------+\n",
      "|{s3://multimedia-...|[31.0,30.0,34.0,1...|\n",
      "|{s3://multimedia-...|[255.0,231.0,195....|\n",
      "|{s3://multimedia-...|[48.0,50.0,51.0,3...|\n",
      "|{s3://multimedia-...|[0.0,0.0,0.0,0.0,...|\n",
      "|{s3://multimedia-...|[0.0,0.0,0.0,0.0,...|\n",
      "+--------------------+--------------------+"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/a/69215982/11262633\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark.ml.linalg import DenseVector, VectorUDT\n",
    "\n",
    "@F.udf(returnType=VectorUDT())\n",
    "def img2vec(image):\n",
    "    try:\n",
    "        image_np = DenseVector(ImageSchema.toNDArray(image).flatten())\n",
    "    except:\n",
    "        image_np = None \n",
    "    return image_np\n",
    "\n",
    "print(f'Image fields = {ImageSchema.imageFields}')\n",
    "df_new = df.withColumn('vecs',img2vec('image')).persist()\n",
    "df_new.show()\n",
    "#df_new.select('vecs').first().asDict().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260aba6f-8c0c-451e-bbea-472f13e6e82a",
   "metadata": {},
   "source": [
    "## List images where the conversion failed\n",
    "\n",
    "*Q:  Slow for large amounts of data.  Why?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6011623-3c78-4754-be3c-71d83aef7843",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T19:07:49.764090Z",
     "iopub.status.busy": "2022-04-21T19:07:49.763914Z",
     "iopub.status.idle": "2022-04-21T19:07:50.561174Z",
     "shell.execute_reply": "2022-04-21T19:07:50.560596Z",
     "shell.execute_reply.started": "2022-04-21T19:07:49.764069Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d79fbccfae2416083a8c323837bbf30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|origin|\n",
      "+------+\n",
      "+------+"
     ]
    }
   ],
   "source": [
    "df_null = df_new.where(df_new.vecs.isNull()).select('image.origin')\n",
    "df_null.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ed6d77-db87-490e-ae7c-97ac673cf1aa",
   "metadata": {},
   "source": [
    "## Set up ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03649357-370c-4670-a379-ca57d4892022",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T19:07:50.562820Z",
     "iopub.status.busy": "2022-04-21T19:07:50.562465Z",
     "iopub.status.idle": "2022-04-21T19:07:50.632875Z",
     "shell.execute_reply": "2022-04-21T19:07:50.630810Z",
     "shell.execute_reply.started": "2022-04-21T19:07:50.562794Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6b8563570442dbab7b1fa750e91f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Helper functions for loading image (hidden)\n",
    "\n",
    "original_image_cache = {}\n",
    "\n",
    "def preprocess_image(image):\n",
    "  image = np.array(image)\n",
    "  # reshape into shape [batch_size, height, width, num_channels]\n",
    "  img_reshaped = tf.reshape(image, [1, image.shape[0], image.shape[1], image.shape[2]])\n",
    "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "  image = tf.image.convert_image_dtype(img_reshaped, tf.float32)\n",
    "  return image\n",
    "\n",
    "def load_image_from_url(img_url):\n",
    "  \"\"\"Returns an image with shape [1, height, width, num_channels].\"\"\"\n",
    "  user_agent = {'User-agent': 'Colab Sample (https://tensorflow.org)'}\n",
    "  response = requests.get(img_url, headers=user_agent)\n",
    "  image = Image.open(BytesIO(response.content))\n",
    "  image = preprocess_image(image)\n",
    "  return image\n",
    "\n",
    "def load_image(image_url, image_size=256, dynamic_size=False, max_dynamic_size=512):\n",
    "  \"\"\"Loads and preprocesses images.\"\"\"\n",
    "  # Cache image file locally.\n",
    "  if image_url in original_image_cache:\n",
    "    img = original_image_cache[image_url]\n",
    "  elif image_url.startswith('https://'):\n",
    "    img = load_image_from_url(image_url)\n",
    "  else:\n",
    "    fd = tf.io.gfile.GFile(image_url, 'rb')\n",
    "    img = preprocess_image(Image.open(fd))\n",
    "  original_image_cache[image_url] = img\n",
    "  # Load and convert to float32 numpy array, add batch dimension, and normalize to range [0, 1].\n",
    "  img_raw = img\n",
    "  if tf.reduce_max(img) > 1.0:\n",
    "    img = img / 255.\n",
    "  if len(img.shape) == 3:\n",
    "    img = tf.stack([img, img, img], axis=-1)\n",
    "  if not dynamic_size:\n",
    "    img = tf.image.resize_with_pad(img, image_size, image_size)\n",
    "  elif img.shape[1] > max_dynamic_size or img.shape[2] > max_dynamic_size:\n",
    "    img = tf.image.resize_with_pad(img, max_dynamic_size, max_dynamic_size)\n",
    "  return img, img_raw\n",
    "\n",
    "def show_image(image, title=''):\n",
    "  image_size = image.shape[1]\n",
    "  w = (image_size * 6) // 320\n",
    "  plt.figure(figsize=(w, w))\n",
    "  plt.imshow(image[0], aspect='equal')\n",
    "  plt.axis('off')\n",
    "  plt.title(title)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a24a526-2026-4260-869c-9217b42d042a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T19:07:50.634541Z",
     "iopub.status.busy": "2022-04-21T19:07:50.634083Z",
     "iopub.status.idle": "2022-04-21T19:07:50.700546Z",
     "shell.execute_reply": "2022-04-21T19:07:50.699878Z",
     "shell.execute_reply.started": "2022-04-21T19:07:50.634503Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aad97a55f824fc886382d4c695b555f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def to_img(row):\n",
    "    row_dict = row.asDict()  \n",
    "    image_dict = row_dict['image'].asDict()\n",
    "    image_data = image_dict['data']\n",
    "    h = image_dict['height']\n",
    "    w = image_dict['width']\n",
    "    c = image_dict['nChannels']\n",
    "    img_b = bytes(image_data)\n",
    "    # https://stackoverflow.com/a/50026948/11262633\n",
    "    img_pil = Image.frombytes('RGB', (h,w), img_b, 'raw')\n",
    "    return img_pil, c\n",
    "\n",
    "def img_to_t(image):\n",
    "    return preprocess_image(image)\n",
    "\n",
    "def to_img_t(row):\n",
    "    image, c = to_img(row)\n",
    "    return img_to_t(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f06fe4-6dac-494e-8b16-b56e24612aac",
   "metadata": {},
   "source": [
    "Hardcoded values for TensorFlow Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed012fa8-b3b4-4dce-8d98-a77d37d54da8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T19:07:50.701633Z",
     "iopub.status.busy": "2022-04-21T19:07:50.701446Z",
     "iopub.status.idle": "2022-04-21T19:07:50.761111Z",
     "shell.execute_reply": "2022-04-21T19:07:50.760300Z",
     "shell.execute_reply.started": "2022-04-21T19:07:50.701611Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f651725be3b461b95cd1d44f52ca165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"efficientnetv2-s\"\n",
    "model_handle = 'https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/classification/2'\n",
    "image_size = 224\n",
    "dynamic_size = False\n",
    "labels_file = \"https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be3f714-5938-4cb6-94b3-942c97fd43d4",
   "metadata": {},
   "source": [
    "Get classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e26e0782-2716-4820-9f4b-59dcde7d9bc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T19:07:50.764985Z",
     "iopub.status.busy": "2022-04-21T19:07:50.762261Z",
     "iopub.status.idle": "2022-04-21T19:07:50.828288Z",
     "shell.execute_reply": "2022-04-21T19:07:50.827597Z",
     "shell.execute_reply.started": "2022-04-21T19:07:50.764946Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5af701e484470a9911f3cc85ac2af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1001 classes"
     ]
    }
   ],
   "source": [
    "#download labels and creates a maps\n",
    "downloaded_file = tf.keras.utils.get_file(\"labels.txt\", origin=labels_file)\n",
    "\n",
    "classes = []\n",
    "\n",
    "with open(downloaded_file) as f:\n",
    "  labels = f.readlines()\n",
    "  classes = [l.strip() for l in labels]\n",
    "print(f'Downloaded {len(classes)} classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13819f3f-650c-4f0f-9c67-c97fbbe0e4da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T19:07:50.831652Z",
     "iopub.status.busy": "2022-04-21T19:07:50.831472Z",
     "iopub.status.idle": "2022-04-21T19:08:06.183203Z",
     "shell.execute_reply": "2022-04-21T19:08:06.182412Z",
     "shell.execute_reply.started": "2022-04-21T19:07:50.831630Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7137ef9c11314b76b52ba118d90713fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = hub.load(model_handle)\n",
    "\n",
    "input_shape = (1, image_size, image_size, 3) #image.shape\n",
    "warmup_input = tf.random.uniform(input_shape, 0, 1.0)\n",
    "warmup_logits = classifier(warmup_input).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063bdc3a-976b-4e36-96d4-e1918c8058e9",
   "metadata": {},
   "source": [
    "## Test locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "319af9a5-faf1-4f75-b08b-cb132e30183c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T19:08:06.184957Z",
     "iopub.status.busy": "2022-04-21T19:08:06.184575Z",
     "iopub.status.idle": "2022-04-21T19:08:08.474786Z",
     "shell.execute_reply": "2022-04-21T19:08:08.472332Z",
     "shell.execute_reply.started": "2022-04-21T19:08:06.184924Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0bffbee2224c81840a9171a47a7ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rows = df.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0752d03-93ed-40b0-933c-257991b3e501",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T19:08:08.476507Z",
     "iopub.status.busy": "2022-04-21T19:08:08.476004Z",
     "iopub.status.idle": "2022-04-21T19:08:08.602167Z",
     "shell.execute_reply": "2022-04-21T19:08:08.601462Z",
     "shell.execute_reply.started": "2022-04-21T19:08:08.476469Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7814220d1049aa880731350a898bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (333, 500) 3\n",
      "1 (375, 500) 3\n",
      "2 (333, 500) 3\n",
      "3 (500, 293) 3\n",
      "4 (260, 500) 3"
     ]
    }
   ],
   "source": [
    "for idx, row in enumerate(rows):\n",
    "    image, c = to_img(row)\n",
    "    print(idx, image.size, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b01ce6d7-b8d7-4960-a65e-ff0355310e87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T19:08:08.623867Z",
     "iopub.status.busy": "2022-04-21T19:08:08.623316Z",
     "iopub.status.idle": "2022-04-21T19:08:28.051043Z",
     "shell.execute_reply": "2022-04-21T19:08:28.050319Z",
     "shell.execute_reply.started": "2022-04-21T19:08:08.623825Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97da0d085a034034a4b0eb47b58d5206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://multimedia-commons/data/images/000/9b1/0009b10615497f91833b17d86281.jpg\n",
      "(1)  905 - window screen: 0.42160648107528687\n",
      "(2)  906 - window shade: 0.2104795128107071\n",
      "(3)  670 - mosquito net: 0.02144552953541279\n",
      "(4)  795 - shower curtain: 0.014206805266439915\n",
      "(5)  754 - radiator: 0.010209481231868267\n",
      "s3://multimedia-commons/data/images/000/4bb/0004bb7bbb2676da9b22642423e90.jpg\n",
      "(1)  905 - window screen: 0.7016111612319946\n",
      "(2)  906 - window shade: 0.042685624212026596\n",
      "(3)  754 - radiator: 0.010777434334158897\n",
      "(4)  800 - sliding door: 0.003164273453876376\n",
      "(5)  557 - fire screen: 0.0031357111874967813\n",
      "s3://multimedia-commons/data/images/000/3d1/0003d13b530ab3dba22749272de6c.jpg\n",
      "(1)  905 - window screen: 0.48613327741622925\n",
      "(2)  906 - window shade: 0.020183252170681953\n",
      "(3)  816 - spider web: 0.013462544418871403\n",
      "(4)  592 - handkerchief: 0.01244745310395956\n",
      "(5)  133 - American egret: 0.01186402142047882\n",
      "s3://multimedia-commons/data/images/000/5e5/0005e54d1faf7b25ccaec519387a.jpg\n",
      "(1)  906 - window shade: 0.12334741652011871\n",
      "(2)  133 - American egret: 0.07040168344974518\n",
      "(3)  978 - sandbar: 0.048677053302526474\n",
      "(4)  905 - window screen: 0.03721398115158081\n",
      "(5)  754 - radiator: 0.033004648983478546"
     ]
    }
   ],
   "source": [
    "# TODO Use a custom function, not a loop\n",
    "for idx, row in enumerate(rows):\n",
    "    \n",
    "    row_dict = row.image.asDict()\n",
    "    \n",
    "    print(row_dict['origin'])\n",
    "    \n",
    "    image = to_img_t(row)\n",
    "    \n",
    "    # Run model on image\n",
    "    probabilities = tf.nn.softmax(classifier(image)).numpy()\n",
    "\n",
    "    top_5 = tf.argsort(probabilities, axis=-1, direction=\"DESCENDING\")[0][:5].numpy()\n",
    "    np_classes = np.array(classes)\n",
    "\n",
    "    # Some models include an additional 'background' class in the predictions, so\n",
    "    # we must account for this when reading the class labels.\n",
    "    includes_background_class = probabilities.shape[1] == 1001\n",
    "\n",
    "    for i, item in enumerate(top_5):\n",
    "      class_index = item if includes_background_class else item + 1\n",
    "      line = f'({i+1}) {class_index:4} - {classes[class_index]}: {probabilities[0][top_5][i]}'\n",
    "      print(line)\n",
    "\n",
    "    #show_image(image, '')\n",
    "    \n",
    "    if idx > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4143e4b-cdb1-4498-8dec-200e0fe46fdc",
   "metadata": {},
   "source": [
    "Demonstrate the ability to call Python functions within Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "befc304d-3f7c-4033-a476-cd8413438303",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T19:08:28.052293Z",
     "iopub.status.busy": "2022-04-21T19:08:28.052045Z",
     "iopub.status.idle": "2022-04-21T19:08:29.320735Z",
     "shell.execute_reply": "2022-04-21T19:08:29.320043Z",
     "shell.execute_reply.started": "2022-04-21T19:08:28.052258Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a61545d59954452a9035dcde7db39aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://multimedia-commons/data/images/000/9b1/0009b10615497f91833b17d86281.jpg\n",
      "s3://multimedia-commons/data/images/000/4bb/0004bb7bbb2676da9b22642423e90.jpg\n",
      "s3://multimedia-commons/data/images/000/3d1/0003d13b530ab3dba22749272de6c.jpg\n",
      "s3://multimedia-commons/data/images/000/5e5/0005e54d1faf7b25ccaec519387a.jpg\n",
      "s3://multimedia-commons/data/images/000/24a/00024a73d1a4c32fb29732d56a2.jpg"
     ]
    }
   ],
   "source": [
    "rdd = df_new.rdd.mapPartitions(lambda iter: [row.image.origin for row in iter])\n",
    "for x in rdd.collect():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00424c7d-95d8-4d36-9cbb-9372dd5eb521",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T19:08:29.322129Z",
     "iopub.status.busy": "2022-04-21T19:08:29.321841Z",
     "iopub.status.idle": "2022-04-21T19:08:36.608432Z",
     "shell.execute_reply": "2022-04-21T19:08:36.607709Z",
     "shell.execute_reply.started": "2022-04-21T19:08:29.322091Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87d016f0ca74d269c31bedaded23f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 500, 333, 3)\n",
      "(1, 500, 375, 3)\n",
      "(1, 500, 333, 3)\n",
      "(1, 293, 500, 3)\n",
      "(1, 500, 260, 3)"
     ]
    }
   ],
   "source": [
    "rdd = df_new.rdd.mapPartitions(lambda iter: [to_img_t(row).shape for row in iter])\n",
    "for x in rdd.collect():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26308c19-debe-403b-8daa-4464cdcd8162",
   "metadata": {},
   "source": [
    "## Build call to ML within Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3b7079-326b-41d3-8224-7576cb7e67e6",
   "metadata": {},
   "source": [
    "A simple UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2228785e-6b33-4e58-ad8e-4f68891ac114",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T19:12:24.305283Z",
     "iopub.status.busy": "2022-04-21T19:12:24.305050Z",
     "iopub.status.idle": "2022-04-21T19:12:25.067323Z",
     "shell.execute_reply": "2022-04-21T19:12:25.066655Z",
     "shell.execute_reply.started": "2022-04-21T19:12:24.305259Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7921eb2b2849a78f0c7b7d86ba7c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(t='s3://multimedia-commons/data/images/000/9b1/0009b10615497f91833b17d86281.jpg')\n",
      "Row(t='s3://multimedia-commons/data/images/000/4bb/0004bb7bbb2676da9b22642423e90.jpg')\n",
      "Row(t='s3://multimedia-commons/data/images/000/3d1/0003d13b530ab3dba22749272de6c.jpg')\n",
      "Row(t='s3://multimedia-commons/data/images/000/5e5/0005e54d1faf7b25ccaec519387a.jpg')\n",
      "Row(t='s3://multimedia-commons/data/images/000/24a/00024a73d1a4c32fb29732d56a2.jpg')"
     ]
    }
   ],
   "source": [
    "_udf = F.udf(lambda image: image.origin)  # Works\n",
    "df_show = df_new.withColumn('t',_udf(df_new['image']))\n",
    "for x in df_show.select('t').collect():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10681e75-bcf0-41ca-b2fd-eceefbc7b943",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T19:18:19.421478Z",
     "iopub.status.busy": "2022-04-21T19:18:19.421232Z",
     "iopub.status.idle": "2022-04-21T19:18:19.486494Z",
     "shell.execute_reply": "2022-04-21T19:18:19.485800Z",
     "shell.execute_reply.started": "2022-04-21T19:18:19.421453Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9c8a65f16c4c579f30c7c5b851bd5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def to_img_np(image):\n",
    "    image_data = image.data\n",
    "    h = image.height\n",
    "    w = image.width\n",
    "    c = image.nChannels\n",
    "    img_b = bytes(image_data)\n",
    "    # https://stackoverflow.com/a/50026948/11262633\n",
    "    img_pil = Image.frombytes('RGB', (h,w), img_b, 'raw')\n",
    "    img_np = np.asarray(img_pil)\n",
    "    return img_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "142b23af-e376-4e27-9e76-0c43104d653e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T19:26:49.186291Z",
     "iopub.status.busy": "2022-04-21T19:26:49.186051Z",
     "iopub.status.idle": "2022-04-21T19:26:49.958078Z",
     "shell.execute_reply": "2022-04-21T19:26:49.957375Z",
     "shell.execute_reply.started": "2022-04-21T19:26:49.186267Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7d6a56fa894d348b1199be878827af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(img_np=DenseVector([500.0, 333.0, 3.0]))\n",
      "Row(img_np=DenseVector([500.0, 375.0, 3.0]))\n",
      "Row(img_np=DenseVector([500.0, 333.0, 3.0]))\n",
      "Row(img_np=DenseVector([293.0, 500.0, 3.0]))\n",
      "Row(img_np=DenseVector([500.0, 260.0, 3.0]))"
     ]
    }
   ],
   "source": [
    "_udf = F.udf(lambda image: DenseVector(to_img_np(image).shape), VectorUDT()) \n",
    "df_show = df_new.withColumn('img_np',_udf(df_new['image']))\n",
    "for x in df_show.select('img_np').collect():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50245683-af90-4830-9e5f-0e4100e19ef3",
   "metadata": {},
   "source": [
    "### Can call TensorFlow utility functions and return scalar results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e2339cdc-9e7a-4db8-858a-e92ba761c5ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T16:39:56.968579Z",
     "iopub.status.busy": "2022-04-21T16:39:56.968375Z",
     "iopub.status.idle": "2022-04-21T16:39:57.734245Z",
     "shell.execute_reply": "2022-04-21T16:39:57.733530Z",
     "shell.execute_reply.started": "2022-04-21T16:39:56.968557Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed0b5a325a94f0e9a80ad7d13670fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(t='1')\n",
      "Row(t='1')\n",
      "Row(t='1')\n",
      "Row(t='1')\n",
      "Row(t='1')"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/44067861/pyspark-add-a-new-column-with-a-tuple-created-from-columns\n",
    "from pyspark.sql.functions import struct\n",
    "\n",
    "def test(data_tuple):\n",
    "  #classifier = hub.load(model_handle)\n",
    "  w = data_tuple[0]\n",
    "  h = data_tuple[1]\n",
    "  c = data_tuple[2]\n",
    "  vecs = data_tuple[3]\n",
    "  # reshape into shape [batch_size, height, width, num_channels]\n",
    "  img_reshaped = tf.reshape(vecs, [1, h, w, c])\n",
    "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "  image = tf.image.convert_image_dtype(img_reshaped, tf.float32)\n",
    "  # Run model on image\n",
    "  #results = classifier(image)\n",
    "  #probabilities = tf.nn.softmax(results).numpy() \n",
    "  #top_5 = tf.argsort(probabilities, axis=-1, direction=\"DESCENDING\")[0][:5].numpy() \n",
    "  #return top_5\n",
    "  return image.shape[0] #probabilities\n",
    "\n",
    "#_udf = F.udf(lambda image: image.origin)  # Works\n",
    "#_udf = F.udf(lambda image: to_img_from_spark(image).size) # Fails - returns string ~ '[Ljava.lang.Object;@38ba1a87'\n",
    "_udf = F.udf(lambda image: test(image)) \n",
    "df_new2 = df_new.withColumn('c',struct(df_new.image.width, df_new.image.height, df_new.image.nChannels, df_new.vecs))\n",
    "df_show = df_new2.withColumn('t',_udf(df_new2['c']))\n",
    "for x in df_show.select('t').collect():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a170b39-1c79-4cb0-a79c-62eb242efda3",
   "metadata": {},
   "source": [
    "### Fails with complex data types\n",
    "\n",
    "Works if return data as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "67bac940-c3bd-44a4-9123-e844177ae9f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T17:45:53.589065Z",
     "iopub.status.busy": "2022-04-21T17:45:53.588837Z",
     "iopub.status.idle": "2022-04-21T17:46:18.919064Z",
     "shell.execute_reply": "2022-04-21T17:46:18.918437Z",
     "shell.execute_reply.started": "2022-04-21T17:45:53.589041Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0dbcb15f3b84e668d5074bd76494b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(t='[478   0   1   2   3]')\n",
      "Row(t='[427   0   1   2   3]')\n",
      "Row(t='[693   0   1   2   3]')\n",
      "Row(t='[237 589 740 585 236]')\n",
      "Row(t='[478 681 155 469   0]')"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/44067861/pyspark-add-a-new-column-with-a-tuple-created-from-columns\n",
    "from pyspark.sql.functions import struct\n",
    "\n",
    "def test(data_tuple, return_string=False):\n",
    "  classifier = hub.load(model_handle)\n",
    "  w = data_tuple[0]\n",
    "  h = data_tuple[1]\n",
    "  c = data_tuple[2]\n",
    "  vecs = data_tuple[3]\n",
    "  # reshape into shape [batch_size, height, width, num_channels]\n",
    "  img_reshaped = tf.reshape(vecs, [1, h, w, c])\n",
    "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "  image = tf.image.convert_image_dtype(img_reshaped, tf.float32)\n",
    "  # Run model on image\n",
    "  results = classifier(image)\n",
    "  probabilities = tf.nn.softmax(results)\n",
    "  top_5 = tf.argsort(probabilities, axis=-1, direction=\"DESCENDING\")[0][:5].numpy() # Shape (5,)\n",
    "  #return top_5\n",
    "  if return_string:\n",
    "    ret_data = str(top_5)\n",
    "  else:\n",
    "    ret_data = top_5\n",
    "  return ret_data  # str(type(probabilities)) results.numpy().shape[0] \n",
    "\n",
    "_udf = F.udf(lambda image: test(image, return_string=True)) \n",
    "df_new2 = df_new.withColumn('c',struct(df_new.image.width, df_new.image.height, df_new.image.nChannels, df_new.vecs))\n",
    "df_show = df_new2.withColumn('t',_udf(df_new2['c']))\n",
    "for x in df_show.select('t').collect():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e59de58-90ef-474b-aa8d-a14d883f3dc8",
   "metadata": {},
   "source": [
    "Same as above but returning top_5, instead of a string version of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "05dd8e0d-5251-4c13-bef1-67001adbb0e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T17:46:42.909523Z",
     "iopub.status.busy": "2022-04-21T17:46:42.909295Z",
     "iopub.status.idle": "2022-04-21T17:47:52.349068Z",
     "shell.execute_reply": "2022-04-21T17:47:52.348334Z",
     "shell.execute_reply.started": "2022-04-21T17:46:42.909500Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a87249918a43bb9159ba474646065c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o1112.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 57.0 failed 4 times, most recent failure: Lost task 4.3 in stage 57.0 (TID 408) (ip-172-31-18-145.us-east-2.compute.internal executor 1): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.core.multiarray._reconstruct)\n",
      "\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n",
      "\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:907)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:359)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2470)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2419)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2418)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2418)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1125)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1125)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1125)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2684)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2626)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2615)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:914)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2241)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2262)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2281)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2306)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:402)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3587)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3751)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3749)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3584)\n",
      "\tat sun.reflect.GeneratedMethodAccessor348.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.core.multiarray._reconstruct)\n",
      "\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n",
      "\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:907)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:359)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 678, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1305, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o1112.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 57.0 failed 4 times, most recent failure: Lost task 4.3 in stage 57.0 (TID 408) (ip-172-31-18-145.us-east-2.compute.internal executor 1): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.core.multiarray._reconstruct)\n",
      "\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n",
      "\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:907)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:359)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2470)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2419)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2418)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2418)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1125)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1125)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1125)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2684)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2626)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2615)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:914)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2241)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2262)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2281)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2306)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:402)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3587)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3751)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3749)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3584)\n",
      "\tat sun.reflect.GeneratedMethodAccessor348.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.core.multiarray._reconstruct)\n",
      "\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n",
      "\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:907)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:359)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_udf = F.udf(lambda image: test(image, return_string=False), returnType=VectorUDT()) \n",
    "df_new2 = df_new.withColumn('c',struct(df_new.image.width, df_new.image.height, df_new.image.nChannels, df_new.vecs))\n",
    "df_show = df_new2.withColumn('t',_udf(df_new2['c']))\n",
    "for x in df_show.select('t').collect():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6b26b07c-12a4-49b0-bc7b-a77ce48a178f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T17:13:24.856853Z",
     "iopub.status.busy": "2022-04-21T17:13:24.856621Z",
     "iopub.status.idle": "2022-04-21T17:13:24.910747Z",
     "shell.execute_reply": "2022-04-21T17:13:24.910044Z",
     "shell.execute_reply.started": "2022-04-21T17:13:24.856829Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7a2ce2a13f4a87818702c6ca3403a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Pandas >= 0.23.2 must be installed; however, it was not found.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py\", line 62, in toPandas\n",
      "    require_minimum_pandas_version()\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/pandas/utils.py\", line 34, in require_minimum_pandas_version\n",
      "    \"it was not found.\" % minimum_pandas_version) from raised_error\n",
      "ImportError: Pandas >= 0.23.2 must be installed; however, it was not found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_show.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1131e5dc-01d3-487e-b63f-d606aac82da2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-21T17:14:06.033845Z",
     "iopub.status.busy": "2022-04-21T17:14:06.033584Z",
     "iopub.status.idle": "2022-04-21T17:15:21.491996Z",
     "shell.execute_reply": "2022-04-21T17:15:21.491434Z",
     "shell.execute_reply.started": "2022-04-21T17:14:06.033819Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb4f7c5c5cc4dbba8252877a22beae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 48.0 failed 4 times, most recent failure: Lost task 4.3 in stage 48.0 (TID 308) (ip-172-31-18-145.us-east-2.compute.internal executor 1): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.core.multiarray._reconstruct)\n",
      "\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n",
      "\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:907)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.to(SerDeUtil.scala:80)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toBuffer(SerDeUtil.scala:80)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toArray(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2281)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2470)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2419)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2418)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2418)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1125)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1125)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1125)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2684)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2626)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2615)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:914)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2241)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2262)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2281)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2306)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.core.multiarray._reconstruct)\n",
      "\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n",
      "\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:907)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.to(SerDeUtil.scala:80)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toBuffer(SerDeUtil.scala:80)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toArray(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2281)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 949, in collect\n",
      "    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1305, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 48.0 failed 4 times, most recent failure: Lost task 4.3 in stage 48.0 (TID 308) (ip-172-31-18-145.us-east-2.compute.internal executor 1): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.core.multiarray._reconstruct)\n",
      "\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n",
      "\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:907)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.to(SerDeUtil.scala:80)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toBuffer(SerDeUtil.scala:80)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toArray(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2281)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2470)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2419)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2418)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2418)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1125)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1125)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1125)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2684)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2626)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2615)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:914)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2241)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2262)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2281)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2306)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.core.multiarray._reconstruct)\n",
      "\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n",
      "\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:907)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.to(SerDeUtil.scala:80)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toBuffer(SerDeUtil.scala:80)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toArray(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2281)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_show.rdd.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
